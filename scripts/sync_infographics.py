#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–åŒæ­¥ä¿¡æ¯å›¾åˆ°GitHub Pages
ä»dataç›®å½•æ‰«æä¿¡æ¯å›¾æ–‡ä»¶ï¼Œæå–å…ƒæ•°æ®ï¼Œæ›´æ–°APIæ–‡ä»¶ï¼Œå¤åˆ¶æ–‡ä»¶åˆ°docsç›®å½•
"""

import os
import re
import json
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from bs4 import BeautifulSoup

class InfographicsSyncer:
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root or os.getcwd())
        self.data_dir = self.project_root / "data"
        self.docs_dir = self.project_root / "docs"
        self.infographics_dir = self.docs_dir / "infographics"
        self.api_file = self.docs_dir / "api" / "infographics.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.infographics_dir.mkdir(parents=True, exist_ok=True)
        self.api_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¸“å®¶åˆ†ç±»æ˜ å°„
        self.expert_categories = {
            "æŠ€æœ¯": "tech",
            "å•†ä¸š": "business", 
            "å“²å­¦": "philosophy",
            "ç§‘å­¦": "science",
            "è®¾è®¡": "design",
            "ç»æµ": "economics"
        }
        
        # å…³é”®è¯åˆ°åˆ†ç±»çš„æ˜ å°„
        self.keyword_to_category = {
            "ai": "tech",
            "å¼€å‘": "tech",
            "ç¨‹åº": "tech",
            "è½¯ä»¶": "tech",
            "ç®—æ³•": "tech",
            "æ•°æ®": "tech",
            "ä¼ä¸š": "business",
            "å•†ä¸š": "business",
            "ç®¡ç†": "business",
            "æˆ˜ç•¥": "business",
            "è¥é”€": "business",
            "å“²å­¦": "philosophy",
            "æ€ç»´": "philosophy",
            "è®¤çŸ¥": "philosophy",
            "ä¼¦ç†": "philosophy",
            "è®¾è®¡": "design",
            "ç”¨æˆ·ä½“éªŒ": "design",
            "ç•Œé¢": "design",
            "å¯è§†åŒ–": "design"
        }
    
    def sync_all(self) -> bool:
        """åŒæ­¥æ‰€æœ‰ä¿¡æ¯å›¾"""
        print("å¼€å§‹åŒæ­¥ä¿¡æ¯å›¾...")
        
        try:
            # 1. æ‰«ædataç›®å½•ä¸­çš„ä¿¡æ¯å›¾æ–‡ä»¶
            infographic_files = self.scan_infographic_files()
            print(f"å‘ç° {len(infographic_files)} ä¸ªä¿¡æ¯å›¾æ–‡ä»¶")
            
            # 2. å¤„ç†æ¯ä¸ªæ–‡ä»¶
            infographics_data = []
            for file_path in infographic_files:
                try:
                    infographic_data = self.process_infographic_file(file_path)
                    if infographic_data:
                        infographics_data.append(infographic_data)
                        print(f"å¤„ç†å®Œæˆ: {file_path.name}")
                except Exception as e:
                    print(f"å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}")
                    continue
            
            # 3. æ›´æ–°APIæ–‡ä»¶
            self.update_api_file(infographics_data)
            print(f"æ›´æ–°APIæ–‡ä»¶: {self.api_file}")
            
            # 4. å¤åˆ¶æ–‡ä»¶åˆ°docsç›®å½•
            self.copy_infographic_files(infographic_files)
            print("å¤åˆ¶æ–‡ä»¶å®Œæˆ")
            
            print(f"åŒæ­¥å®Œæˆ! å…±å¤„ç† {len(infographics_data)} ä¸ªä¿¡æ¯å›¾")
            return True
            
        except Exception as e:
            print(f"åŒæ­¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return False
    
    def scan_infographic_files(self) -> List[Path]:
        """æ‰«ædataç›®å½•ä¸­çš„ä¿¡æ¯å›¾HTMLæ–‡ä»¶"""
        if not self.data_dir.exists():
            raise FileNotFoundError(f"Dataç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
        
        files = []
        for file_path in self.data_dir.glob("*.html"):
            # åªå¤„ç†ä¿¡æ¯å›¾æ–‡ä»¶
            if ("infographic" in file_path.name.lower() or 
                "expert_debate" in file_path.name.lower()):
                files.append(file_path)
        
        return sorted(files, key=lambda x: x.stat().st_mtime, reverse=True)
    
    def process_infographic_file(self, file_path: Path) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªä¿¡æ¯å›¾æ–‡ä»¶ï¼Œæå–å…ƒæ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # æå–åŸºæœ¬ä¿¡æ¯
            title = self.extract_title(soup, file_path)
            question = self.extract_question(soup)
            experts = self.extract_experts(soup)
            
            # ç”ŸæˆID
            infographic_id = self.generate_id(file_path)
            
            # è·å–æ–‡ä»¶æ—¶é—´
            file_stat = file_path.stat()
            created_time = datetime.fromtimestamp(file_stat.st_ctime)
            updated_time = datetime.fromtimestamp(file_stat.st_mtime)
            
            # åˆ†æå†…å®¹
            summary = self.generate_summary(title, question, experts)
            category = self.determine_category(title, question, experts)
            tags = self.extract_tags(title, question, content)
            word_count = self.count_words(content)
            
            return {
                "id": infographic_id,
                "title": title,
                "question": question,
                "date": created_time.strftime("%Y-%m-%d"),
                "created": created_time.isoformat() + "Z",
                "updated": updated_time.isoformat() + "Z",
                "filename": file_path.name,
                "path": f"infographics/{file_path.name}",
                "experts": experts,
                "summary": summary,
                "category": category,
                "tags": tags,
                "rounds": 4,  # é»˜è®¤4è½®
                "wordCount": word_count,
                "status": "completed",
                "featured": self.is_featured(file_path)
            }
            
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}")
            return None
    
    def extract_title(self, soup: BeautifulSoup, file_path: Path) -> str:
        """æå–æ ‡é¢˜"""
        # å°è¯•ä»titleæ ‡ç­¾æå–
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text().strip()
            # æ¸…ç†æ ‡é¢˜
            title = re.sub(r'\s*-\s*.*$', '', title)  # ç§»é™¤å‰¯æ ‡é¢˜
            if title and title != "ä¸“å®¶è¾©è®ºä¿¡æ¯å›¾":
                return title
        
        # å°è¯•ä»h1æ ‡ç­¾æå–
        h1_tag = soup.find('h1')
        if h1_tag:
            title = h1_tag.get_text().strip()
            if title:
                return title
        
        # å°è¯•ä».questionç±»æå–
        question_div = soup.find(class_='question')
        if question_div:
            question = question_div.get_text().strip()
            if question:
                return question
        
        # ä»æ–‡ä»¶åç”Ÿæˆ
        return self.generate_title_from_filename(file_path)
    
    def extract_question(self, soup: BeautifulSoup) -> str:
        """æå–é—®é¢˜"""
        # æŸ¥æ‰¾é—®é¢˜ç›¸å…³çš„å…ƒç´ 
        question_selectors = [
            '.question',
            '.header .question',
            'h1',
            '.title'
        ]
        
        for selector in question_selectors:
            element = soup.select_one(selector)
            if element:
                text = element.get_text().strip()
                if text and '?' in text:
                    return text
        
        return "ä¸“å®¶è¾©è®ºé—®é¢˜"
    
    def extract_experts(self, soup: BeautifulSoup) -> List[Dict]:
        """æå–ä¸“å®¶ä¿¡æ¯"""
        experts = []
        
        # æŸ¥æ‰¾ä¸“å®¶ç›¸å…³çš„å…ƒç´ 
        expert_elements = soup.select('.expert, .expert-name, .persona')
        
        for element in expert_elements:
            # å°è¯•æå–ä¸“å®¶åç§°å’Œemoji
            name_elem = element.find(class_='expert-name') or element
            emoji_elem = element.find(class_='expert-emoji') or element
            
            if name_elem:
                name = name_elem.get_text().strip()
                emoji = "ğŸ§ "  # é»˜è®¤emoji
                
                # å°è¯•æå–emoji
                if emoji_elem:
                    emoji_text = emoji_elem.get_text().strip()
                    emoji_match = re.search(r'([ğŸ§ ğŸ’¡ğŸ¯ğŸ—ï¸âš›ï¸ğŸ’»ğŸ“ŠğŸ¨ğŸ“ˆğŸ”ğŸ’­ğŸ¤–ğŸ“])', emoji_text)
                    if emoji_match:
                        emoji = emoji_match.group(1)
                
                if name and name not in [e['name'] for e in experts]:
                    experts.append({
                        "name": name,
                        "emoji": emoji,
                        "role": self.determine_expert_role(name),
                        "category": self.determine_expert_category(name)
                    })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸“å®¶ï¼Œæ·»åŠ é»˜è®¤ä¸“å®¶
        if not experts:
            experts = [
                {"name": "ä¸“å®¶A", "emoji": "ğŸ§ ", "role": "é¢†åŸŸä¸“å®¶", "category": "tech"},
                {"name": "ä¸“å®¶B", "emoji": "ğŸ’¡", "role": "æ€æƒ³å®¶", "category": "philosophy"},
                {"name": "ä¸“å®¶C", "emoji": "ğŸ¯", "role": "å®è·µè€…", "category": "business"}
            ]
        
        return experts[:3]  # æœ€å¤š3ä¸ªä¸“å®¶
    
    def determine_expert_role(self, name: str) -> str:
        """æ ¹æ®ä¸“å®¶åç§°ç¡®å®šè§’è‰²"""
        role_mapping = {
            "é©¬ä¸Â·ç¦å‹’": "è½¯ä»¶æ¶æ„å¸ˆ",
            "ä¸¹Â·é˜¿å¸ƒæ‹‰è«å¤«": "å‰ç«¯ä¸“å®¶",
            "æ°å¤«Â·é˜¿ç‰¹ä¼å¾·": "è½¯ä»¶å·¥ç¨‹å¸ˆ",
            "è¿ˆå…‹å°”Â·æ³¢ç‰¹": "æˆ˜ç•¥ç®¡ç†ä¸“å®¶",
            "å…‹é‡Œæ–¯æ»•æ£®": "åˆ›æ–°ç†è®ºå®¶",
            "å®‰è¿ªÂ·æ ¼é²å¤«": "ä¼ä¸šç®¡ç†ä¸“å®¶",
            "çˆ±å¾·åÂ·å¡”å¤«ç‰¹": "ä¿¡æ¯è®¾è®¡å¤§å¸ˆ",
            "å”çº³å¾·Â·è¯ºæ›¼": "è®¾è®¡å¿ƒç†å­¦å®¶",
            "é˜¿å°”è´æ‰˜Â·å¼€ç½—": "æ•°æ®å¯è§†åŒ–ä¸“å®¶"
        }
        
        return role_mapping.get(name, "é¢†åŸŸä¸“å®¶")
    
    def determine_expert_category(self, name: str) -> str:
        """æ ¹æ®ä¸“å®¶åç§°ç¡®å®šåˆ†ç±»"""
        category_mapping = {
            "é©¬ä¸Â·ç¦å‹’": "tech",
            "ä¸¹Â·é˜¿å¸ƒæ‹‰è«å¤«": "tech",
            "æ°å¤«Â·é˜¿ç‰¹ä¼å¾·": "tech",
            "è¿ˆå…‹å°”Â·æ³¢ç‰¹": "business",
            "å…‹é‡Œæ–¯æ»•æ£®": "business",
            "å®‰è¿ªÂ·æ ¼é²å¤«": "business",
            "çˆ±å¾·åÂ·å¡”å¤«ç‰¹": "design",
            "å”çº³å¾·Â·è¯ºæ›¼": "design",
            "é˜¿å°”è´æ‰˜Â·å¼€ç½—": "design"
        }
        
        return category_mapping.get(name, "philosophy")
    
    def generate_summary(self, title: str, question: str, experts: List[Dict]) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        expert_names = [e['name'] for e in experts]
        if len(expert_names) > 2:
            expert_str = f"{expert_names[0]}ã€{expert_names[1]}ç­‰{len(expert_names)}ä½ä¸“å®¶"
        else:
            expert_str = "ã€".join(expert_names)
        
        return f"{expert_str}å°±{title}å±•å¼€æ·±å…¥è®¨è®ºï¼Œæ¢è®¨ç›¸å…³é¢†åŸŸçš„æ ¸å¿ƒé—®é¢˜ä¸è§£å†³æ–¹æ¡ˆã€‚"
    
    def determine_category(self, title: str, question: str, experts: List[Dict]) -> str:
        """ç¡®å®šåˆ†ç±»"""
        text = f"{title} {question}".lower()
        
        # ç»Ÿè®¡ä¸“å®¶åˆ†ç±»
        expert_categories = [e['category'] for e in experts]
        if expert_categories:
            # è¿”å›ä¸“å®¶ä¸­æœ€å¸¸è§çš„åˆ†ç±»
            from collections import Counter
            category_counter = Counter(expert_categories)
            return category_counter.most_common(1)[0][0]
        
        # åŸºäºå…³é”®è¯ç¡®å®šåˆ†ç±»
        for keyword, category in self.keyword_to_category.items():
            if keyword in text:
                return category
        
        return "philosophy"  # é»˜è®¤åˆ†ç±»
    
    def extract_tags(self, title: str, question: str, content: str) -> List[str]:
        """æå–æ ‡ç­¾"""
        tags = set()
        text = f"{title} {question}".lower()
        
        # å»ºç­‘ä¸è®¾è®¡ç›¸å…³æ ‡ç­¾
        architecture_keywords = {
            "å»ºç­‘": "å»ºç­‘",
            "è®¾è®¡": "è®¾è®¡", 
            "å±•é¦†": "å±•é¦†",
            "ä¸‡åš": "ä¸‡åš",
            "ç©ºé—´": "ç©ºé—´è®¾è®¡",
            "ç¾å­¦": "ç¾å­¦",
            "è‰ºæœ¯": "è‰ºæœ¯"
        }
        for keyword, tag in architecture_keywords.items():
            if keyword in text:
                tags.add(tag)
        
        # æŠ€æœ¯ç›¸å…³æ ‡ç­¾
        tech_keywords = {
            "ai": "AI",
            "äººå·¥æ™ºèƒ½": "AI",
            "å¼€å‘": "å¼€å‘",
            "è½¯ä»¶": "è½¯ä»¶",
            "ç¨‹åº": "ç¼–ç¨‹",
            "ç®—æ³•": "ç®—æ³•",
            "æ•°æ®": "æ•°æ®",
            "æŠ€æœ¯": "æŠ€æœ¯",
            "æ¶æ„": "è½¯ä»¶æ¶æ„",
            "ç å†œ": "ç¨‹åºå‘˜"
        }
        for keyword, tag in tech_keywords.items():
            if keyword in text:
                tags.add(tag)
        
        # å•†ä¸šç›¸å…³æ ‡ç­¾
        business_keywords = {
            "ä¼ä¸š": "ä¼ä¸š",
            "å•†ä¸š": "å•†ä¸š",
            "ç®¡ç†": "ç®¡ç†",
            "æˆ˜ç•¥": "æˆ˜ç•¥",
            "è¥é”€": "è¥é”€",
            "æ•°å­—åŒ–": "æ•°å­—åŒ–",
            "è½¬å‹": "è½¬å‹"
        }
        for keyword, tag in business_keywords.items():
            if keyword in text:
                tags.add(tag)
        
        # æ–‡åŒ–ä¸åœ°åŸŸæ ‡ç­¾
        culture_keywords = {
            "æ—¥æœ¬": "æ—¥æœ¬",
            "å¤§é˜ª": "å¤§é˜ª",
            "ä¸­å›½": "ä¸­å›½",
            "æ–‡åŒ–": "æ–‡åŒ–",
            "ä¼ ç»Ÿ": "ä¼ ç»Ÿ",
            "ç°ä»£": "ç°ä»£"
        }
        for keyword, tag in culture_keywords.items():
            if keyword in text:
                tags.add(tag)
        
        # å…¶ä»–ä¸“ä¸šé¢†åŸŸæ ‡ç­¾
        other_keywords = {
            "å“²å­¦": "å“²å­¦",
            "å¿ƒç†": "å¿ƒç†å­¦",
            "ç»æµ": "ç»æµ",
            "ç§‘å­¦": "ç§‘å­¦",
            "æ•™è‚²": "æ•™è‚²",
            "åˆ›æ–°": "åˆ›æ–°"
        }
        for keyword, tag in other_keywords.items():
            if keyword in text:
                tags.add(tag)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ‡ç­¾ï¼Œæ ¹æ®é—®é¢˜ç±»å‹æ·»åŠ é»˜è®¤æ ‡ç­¾
        if not tags:
            if "?" in question or "ï¼Ÿ" in question:
                tags.add("ä¸“å®¶è¾©è®º")
        
        # é™åˆ¶æ ‡ç­¾æ•°é‡å¹¶æ’åº
        return sorted(list(tags))[:8]
    
    def count_words(self, content: str) -> int:
        """ç»Ÿè®¡å­—æ•°"""
        # ç§»é™¤HTMLæ ‡ç­¾
        soup = BeautifulSoup(content, 'html.parser')
        text = soup.get_text()
        
        # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ•°
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        return len(chinese_chars)
    
    def is_featured(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç²¾é€‰"""
        # å¯ä»¥æ ¹æ®æ–‡ä»¶å¤§å°ã€åˆ›å»ºæ—¶é—´ç­‰åˆ¤æ–­
        return "sample" in file_path.name.lower()
    
    def generate_id(self, file_path: Path) -> str:
        """ç”Ÿæˆä¿¡æ¯å›¾ID"""
        # å°è¯•ä»æ–‡ä»¶åæå–ID
        name = file_path.stem
        if "_" in name:
            parts = name.split("_")
            if len(parts) > 1:
                return parts[-1]
        
        # ç”ŸæˆåŸºäºæ–‡ä»¶åçš„ID
        return name.replace("infographic_", "").replace("expert_debate_", "")
    
    def generate_title_from_filename(self, file_path: Path) -> str:
        """ä»æ–‡ä»¶åç”Ÿæˆæ ‡é¢˜"""
        name = file_path.stem
        
        # ç§»é™¤å¸¸è§å‰ç¼€
        name = name.replace("infographic_", "").replace("expert_debate_", "")
        
        # å¤„ç†ç‰¹æ®Šåç§°
        if "sample" in name.lower():
            return "ä¸“å®¶è¾©è®ºä¿¡æ¯å›¾æ ·ä¾‹"
        elif "gemini" in name.lower():
            return "AIæ¨¡å‹èƒ½åŠ›å¯¹æ¯”åˆ†æ"
        elif "sonet" in name.lower():
            return "AIæ¨ç†èƒ½åŠ›å‘å±•è¶‹åŠ¿"
        
        return f"ä¸“å®¶è¾©è®º: {name}"
    
    def update_api_file(self, infographics_data: List[Dict]) -> None:
        """æ›´æ–°APIæ–‡ä»¶"""
        # ç”Ÿæˆå…ƒæ•°æ®
        categories = {}
        all_tags = set()
        experts_count = 0
        
        for item in infographics_data:
            # ç»Ÿè®¡åˆ†ç±»
            category = item['category']
            categories[category] = categories.get(category, 0) + 1
            
            # æ”¶é›†æ ‡ç­¾
            all_tags.update(item['tags'])
            
            # ç»Ÿè®¡ä¸“å®¶
            experts_count += len(item['experts'])
        
        metadata = {
            "total": len(infographics_data),
            "lastUpdated": datetime.now().isoformat() + "Z",
            "categories": categories,
            "experts": {
                "total": experts_count,
                "unique": len(set(e['name'] for item in infographics_data for e in item['experts']))
            },
            "tags": sorted(list(all_tags))
        }
        
        # æ„å»ºAPIæ•°æ®
        api_data = {
            "infographics": infographics_data,
            "metadata": metadata
        }
        
        # å†™å…¥æ–‡ä»¶
        with open(self.api_file, 'w', encoding='utf-8') as f:
            json.dump(api_data, f, ensure_ascii=False, indent=2)
    
    def copy_infographic_files(self, file_paths: List[Path]) -> None:
        """å¤åˆ¶ä¿¡æ¯å›¾æ–‡ä»¶åˆ°docsç›®å½•ï¼Œå¹¶æ¸…ç†ä¸å­˜åœ¨çš„æ–‡ä»¶"""
        # è·å–å½“å‰å­˜åœ¨çš„æ–‡ä»¶å
        current_files = {file_path.name for file_path in file_paths}
        
        # æ¸…ç†docsç›®å½•ä¸­ä¸å†å­˜åœ¨çš„HTMLæ–‡ä»¶
        self.cleanup_removed_files(current_files)
        
        # å¤åˆ¶å½“å‰æ–‡ä»¶
        for file_path in file_paths:
            dest_path = self.infographics_dir / file_path.name
            shutil.copy2(file_path, dest_path)
            print(f"å¤åˆ¶æ–‡ä»¶: {file_path.name} -> {dest_path}")
    
    def cleanup_removed_files(self, current_files: set) -> None:
        """æ¸…ç†docsç›®å½•ä¸­ä¸å†å­˜åœ¨çš„ä¿¡æ¯å›¾æ–‡ä»¶"""
        if not self.infographics_dir.exists():
            return
        
        # æ‰«ædocs/infographicsç›®å½•ä¸­çš„HTMLæ–‡ä»¶
        existing_files = []
        for file_path in self.infographics_dir.glob("*.html"):
            # è·³è¿‡index.htmlç­‰éä¿¡æ¯å›¾æ–‡ä»¶
            if file_path.name != "index.html":
                existing_files.append(file_path)
        
        # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„æ–‡ä»¶
        files_to_remove = []
        for file_path in existing_files:
            if file_path.name not in current_files:
                files_to_remove.append(file_path)
        
        # åˆ é™¤ä¸å†å­˜åœ¨çš„æ–‡ä»¶
        for file_path in files_to_remove:
            try:
                file_path.unlink()
                print(f"åˆ é™¤æ–‡ä»¶: {file_path.name} (æºæ–‡ä»¶å·²ä¸å­˜åœ¨)")
            except Exception as e:
                print(f"åˆ é™¤æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='åŒæ­¥ä¿¡æ¯å›¾åˆ°GitHub Pages')
    parser.add_argument('--project-root', default='.', help='é¡¹ç›®æ ¹ç›®å½•')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…æ‰§è¡Œ')
    
    args = parser.parse_args()
    
    syncer = InfographicsSyncer(args.project_root)
    
    if args.dry_run:
        print("é¢„è§ˆæ¨¡å¼ï¼š")
        files = syncer.scan_infographic_files()
        for file_path in files:
            print(f"  - {file_path.name}")
    else:
        success = syncer.sync_all()
        exit(0 if success else 1)

if __name__ == "__main__":
    main()